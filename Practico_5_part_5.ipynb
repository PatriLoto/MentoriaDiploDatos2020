{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practico 5 (parte 5)\n",
    "\n",
    "## Visualizar los word embeddings que entrenamos\n",
    "\n",
    "* [Importación de módulos y librerías](#Importación-de-módulos-y-librerías)\n",
    "* [Definición de que vamos a mostrar del embedding y como vamos a hacerlo](#Definición-de-que-vamos-a-mostrar-del-embedding-y-como-vamos-a-hacerlo)\n",
    "* [Embedding Word2Vec (caso 1)](#Embedding-Word2Vec-(caso-1))\n",
    "* [Embedding Word2Vec (caso 2)](#Embedding-Word2Vec-(caso-2))\n",
    "* [Embedding Word2Vec (caso 3)](#Embedding-Word2Vec-(caso-3))\n",
    "\n",
    "## Importación de módulos y librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Inclusion de librerias y módulos\n",
    "import os\n",
    "import logging\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Algunas utilidades\n",
    "from utiles import bcolors\n",
    "\n",
    "# Importamos wrod2vec de la lib gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "# Importamos PCA de sklear\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Importamos logger para tener informacion de estado\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Fijamos el estilo a seaborn\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "# Fijamos la semilla del random\n",
    "np.random.seed(42)\n",
    "\n",
    "# Establecimos el muestreo del vocabulario\n",
    "Nsamples = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de que vamos a mostrar del embedding y como vamos a hacerlo\n",
    "\n",
    "Por cada embeding que tenemos vamos a presentar:\n",
    "\n",
    "* El nombre del archivo que lo contiene, el cual tiene codificado los hiperparametros como se presento en ejercicio anterior\n",
    "> YYYYMMDD-hhmmss_model_{size}-{window}-{min_count}-{sg}.bin\n",
    "> * **size**: (predeterminado 100) El número de dimensiones del embedding, por ej. la longitud del vector denso para representar cada token (palabra).\n",
    "> * **window**: (predeterminado 5) La distancia máxima entre una palabra de referencia y las palabras alrededor de la palabra de referencia.\n",
    "> * **min_count**: (predeterminado 5) El recuento mínimo de palabras a considerar al entrenar el modelo; las palabras con una ocurrencia menor que este recuento serán ignoradas.\n",
    "> * **sg**: (predeterminado 0 o CBOW) El algoritmo de entrenamiento, ya sea CBOW (0) o salto gramo (1).\n",
    "* El tamaño del vocabulario que maneja \n",
    "* Las primeras 50 palabras que posee\n",
    "* Las últimas 50 palabras que posee\n",
    "* Un scatter plot de un sub grupo de `Nsamples` palabras del embedding, un conjunto de palabras con connotación positiva y un conjunto de emojis con connotación positiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embeding(modelfile, Nsamples=200, positives=None, emojis=None, useglove=False):\n",
    "    #############################\n",
    "    # Cargamos el embedding de preferencia, incluyendo la logica para Glove\n",
    "    if useglove:\n",
    "        model =  KeyedVectors.load_word2vec_format(modelfile, binary=False)\n",
    "    else:\n",
    "        model = Word2Vec.load(modelfile)\n",
    "    \n",
    "    #############################\n",
    "    # Imprimimos el caso nombre del archivo del embedding, el tamaño del vocabulario\n",
    "    # las primeras 50 palabras del vocabulario y la últimas 50 palabras del vocabulario\n",
    "    print(f'{bcolors.HEADER}{modelfile} - Tamaño del vocabulario {len(list(model.wv.vocab))}{bcolors.ENDC}')\n",
    "    print(f'{bcolors.OKGREEN}Primeras 50 compoentes del vocabulario{bcolors.ENDC}')\n",
    "    print(f'{bcolors.OKBLUE}{list(model.wv.vocab.keys())[0:50]}{bcolors.ENDC}')\n",
    "    print(f'{bcolors.OKGREEN}Ultimas 50 compoentes del vocabulario{bcolors.ENDC}')\n",
    "    print(f'{bcolors.OKBLUE}{list(model.wv.vocab.keys())[-50:]}{bcolors.ENDC}')\n",
    "    print(f'{bcolors.FAIL}#######################################{bcolors.ENDC}')\n",
    "\n",
    "\n",
    "    #############################\n",
    "    # Se genera el embedding de cada palabra del vocabulario\n",
    "    X = model[model.wv.vocab]\n",
    "    \n",
    "    # Se genera el embedding de las palabras con connotación positiva\n",
    "    if positives: Xp = model[positives]\n",
    "    \n",
    "    # Se genera el embedding de los emojis con connotación positiva\n",
    "    if emojis: Xe = model[emojis]\n",
    "\n",
    "    # Se hace reducción de dimensionalidad por medio de PCA\n",
    "    # sobre los embeddings generados\n",
    "    pca = PCA(n_components=2)\n",
    "    result = pca.fit_transform(X)\n",
    "    if positives: resultp = pca.transform(Xp)\n",
    "    if emojis: resulte = pca.transform(Xe)\n",
    "\n",
    "\n",
    "    #############################\n",
    "    # create a scatter plot of the projection\n",
    "    index = np.random.choice(range(0, result.shape[0]), Nsamples, replace=False)\n",
    "    \n",
    "    # Plot de los embeddings luego de aplicar PCA\n",
    "    plt.figure(figsize=(10,6))\n",
    "    p1 = sns.scatterplot(x=result[index, 0], y=result[index, 1], s=40, label='Vocabulario')\n",
    "    if positives: p2 = sns.scatterplot(x=resultp[:, 0], y=resultp[:, 1], s=80, color='red', label='Palabras positivas')\n",
    "    if emojis: p3 = sns.scatterplot(x=resulte[:, 0], y=resulte[:, 1], s=80, color='green', label='Emojis positivos')\n",
    "\n",
    "    words = np.array(list(model.wv.vocab))[index]\n",
    "    result = result[index,:]\n",
    "    for i,word in enumerate(words):\n",
    "        p1.text(result[i,0]+0.1, result[i,1], word,horizontalalignment='left', size='medium', color='black')#, weight='semibold')\n",
    "    \n",
    "    if positives:\n",
    "        for i,word in enumerate(positives):\n",
    "            p2.text(resultp[i,0]+0.1, resultp[i,1], word,horizontalalignment='left', size='large', color='red')#, weight='semibold')\n",
    "    \n",
    "    if emojis:\n",
    "        for i,word in enumerate(emojis):\n",
    "            p3.text(resulte[i,0]+0.1, resulte[i,1], word,horizontalalignment='left', size='large', color='green')#, weight='semibold')\n",
    "\n",
    "    plt.xlabel('X', fontsize=14)\n",
    "    plt.ylabel('Y', fontsize=14)\n",
    "    plt.title('Word2Vec + PCA', fontsize=14)\n",
    "    plt.legend(fontsize=14)\n",
    "    sns.despine()\n",
    "    plt.show()\n",
    "#     plt.savefig(modelfile+'.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de lista de palabras y emojis con connotación positiva\n",
    "\n",
    "* La variable `positives` contine la lista de palabras asociada a expresiones positivas\n",
    "* La variable `emojis` contiene los emojis asociados a expresiones positivas\n",
    "\n",
    "Es necesario destacar que no hemos considerado emoticones en el análisis dado que su extracción se hace compleja en el contexto de este tipo de diálogos que contiene expresiones algebráicas. Si bien no es una tarea imposible de realizar, el factor tiempo no hace posible abordar el tratamiento adecuado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives = list()\n",
    "positives.append('great')\n",
    "positives.append('excellent')\n",
    "positives.append('nice')\n",
    "positives.append('awesome')\n",
    "positives.append('amazing')\n",
    "positives.append('fantastic')\n",
    "positives.append('wonderful')\n",
    "positives.append('perfect')\n",
    "positives.append('nailed')\n",
    "positives.append('brilliant')\n",
    "positives.append('lovely')\n",
    "positives.append('superb')\n",
    "positives.append('beautiful')\n",
    "positives.append('yupppppp')\n",
    "\n",
    "emojis = list()\n",
    "emojis.append(':grinning_face_with_smiling_eyes:')\n",
    "emojis.append(':smiling_face_with_smiling_eyes:')\n",
    "emojis.append(':face_with_tears_of_joy:')\n",
    "emojis.append(':grinning_face:')\n",
    "emojis.append(':smiling_face_with_sunglasses:')\n",
    "emojis.append(':smiling_face_with_open_mouth_&_smiling_eyes:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Word2Vec (caso 1)\n",
    "\n",
    "Hiperparámetros:\n",
    " \n",
    " | size | window | min_count | sg |\n",
    " | --- | --- | --- | --- |\n",
    " | 100  | 5      | 1         | 0 |\n",
    "\n",
    "En el presente caso nos intereesa analizar el impacto del hiperparámetro `min_count` en la representación del vocabulario. En primera instancia, esperamos que el vocabulario sea extenso o al menos el mayor de todos los casos que se analizaran en el presente notebook. Esto es así dado que dicho hiperparámetro controla la frecuencia mínima que debe tener una palabra del vocabulario para poder ser contemplada en el proceso de embedding. Dado que `min_count = 1`, todas las palabras son consideradas y el vocabulario del embedding es de 91791 palabras.\n",
    "\n",
    "Si observamos las 50 últimas palabras que aparecen en la parte final del vocabulario (ordenadas de mayor a menor segun la frecuencia de ocurrencia) la mayoria se corresponden con expresiones matemáticas (ecuaciones) y algunos nombres personales, probablemente pertenecientes al enunciado de un problema de matemáticas.\n",
    "\n",
    "Si observamos el scatter plot del embedding para aun un sub conjunto de `Nsamples = 1000` palabras podemos identificar en el centro palabras relacionadas a expresiones matemáticas, tecnicismos, nombres personales entre otras. Sin embargo, las palabras y emojis con connotacion positiva se encuentra bien distanciadas del resto. Esto, a su vez teniendo en cuenta que es una proyección bidimensionoal aplicando PCA. Lo que sugiere que en el espacio R$^{100}$ esta separación podria ser mas evidente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95m./20201008-140336_model_100-5-1-0.bin - Tamaño del vocabulario 91791\u001b[0m\n",
      "\u001b[92mPrimeras 50 compoentes del vocabulario\u001b[0m\n",
      "\u001b[94m['<url>', 'hey', 'robert', '!', 'welcome', 'yup', '', 'looking', 'problem', \"'ve\", 'reviewed', 'finding', 'domain', 'square', 'root', 'function', 'let', \"'s\", 'work', 'together', 'find', 'exactly', \"'re\", 'stuck', 'tried', '?', 'simplifying', 'wrong', 'okay', 'actually', 'need', 'simplify', 'however', 'would', \"n't\", 'please', 'show', 'check', '2x√-x+7', 'appreciate', 'seem', 'relevant', 'discuss', 'move', 'tell', 'mean', 'word', '\"', 'meant', 'learned']\u001b[0m\n",
      "\u001b[92mUltimas 50 compoentes del vocabulario\u001b[0m\n",
      "\u001b[94m['x/-4', 'x/-4>8', '880÷.20', '80÷20=40', 'johnathon', '3÷6=2', '2but', 'hermela', '738', 'karia', '800+.2', 'antonniets', 'utillities', 'bill(which', '.1×5', 'questoin', 'dook', '37.5×.1=', 'netpay', 'pay(880', '44is', '3,75', '47.75', '×3', '143.25', 'naything', 'z+1)^2', 'z+1)(z+1', '2z+1', '16z+8', '8z+12', '8z+6', '2z+3', '2*x+0', '2(0)+(0)-3(0)+4', '-j-6', '-3j+4', 'upyo', '-4x-16', 'y>7', 'o,0', '49>48', 'x>48', '-3(1)+4', '-(1-x)+3', '-(1)+3', 'sotiria', '2(s', 'utitlities', '176.00']\u001b[0m\n",
      "\u001b[91m#######################################\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2027dc652f9b4ad69757c487d6e48fbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "modelfile = os.path.join('.','20201008-140336_model_100-5-1-0.bin')\n",
    "plot_embeding(modelfile, Nsamples=1000, positives=positives, emojis=emojis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Word2Vec (caso 2)\n",
    "\n",
    "Hiperparámetros:\n",
    " \n",
    " | size | window | min_count | sg |\n",
    " | --- | --- | --- | --- |\n",
    " | 100  | 5      | 100       | 0 |\n",
    " \n",
    "En el presente caso nos intereesa analizar el impacto del hiperparámetro `min_count` en la representación del vocabulario. En primera instancia, esperamos que el vocabulario significativamente menor que el recientemente analizado debido a que la frecuencia mínima que debe tener una palabra para poder ser contemplada en el proceso de embedding es `min_count = 100`. Dicho esto, el vocabulario del embedding es de 2591 palabras.\n",
    "\n",
    "Si observamos las 50 últimas palabras que aparecen en la parte final del vocabulario la mayoria ya no se corresponden con expresiones matemáticas (ecuaciones), ahora aparece palabras mas generales.\n",
    "\n",
    "\n",
    "\n",
    "Si observamos el scatter plot del embedding par aun un sub conjunto de `Nsamples = 200` palabras podemos identificar nuevamente en el centro un conglomerado palabras pero con significados diferentes. A su vez, las palabras y emojis con connotacion positiva se encuentra mas proximas a este conglomerado. Sin embarogo, las palabras del sub conjunto positivo y de emojis se encuentran muy próximas entre si."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95m./20201008-140436_model_100-5-100-0.bin - Tamaño del vocabulario 2591\u001b[0m\n",
      "\u001b[92mPrimeras 50 compoentes del vocabulario\u001b[0m\n",
      "\u001b[94m['<url>', 'hey', '!', 'welcome', 'yup', '', 'looking', 'problem', \"'ve\", 'reviewed', 'finding', 'domain', 'square', 'root', 'function', 'let', \"'s\", 'work', 'together', 'find', 'exactly', \"'re\", 'stuck', 'tried', '?', 'simplifying', 'wrong', 'okay', 'actually', 'need', 'simplify', 'however', 'would', \"n't\", 'please', 'show', 'check', 'appreciate', 'seem', 'discuss', 'move', 'tell', 'mean', 'word', '\"', 'meant', 'learned', 'far', 'x', 'good']\u001b[0m\n",
      "\u001b[92mUltimas 50 compoentes del vocabulario\u001b[0m\n",
      "\u001b[94m['opens', 'neutrons', 'varma', 'pages', 'x/2', '):', 'electron', 'h+', 'ff', 'collinear', 'eliza', 'terminal', 'amber', 'bonds', 'ticket', 'tickets', 'bonding', 'delta', 'natalie', 'joe', 'breanna', 'f(1', 'molecule', 'pizza', 'matrices', 'oscar', 'wavelength', 'valence', 'slices', 'mackenzie', 'definite', 'yeap', 'index', 'riley', 'dipole', 'shinde', 'candaza', 'alonzo', 'okaye', 'dora', 'vavilakolanu', 'fn', 'kothari', ':-d', 'krystal', 'robin', 'sarada', 'problem-', 'divahar', 'herring']\u001b[0m\n",
      "\u001b[91m#######################################\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e81558ff9e4d4095a8bf2f8ec179bf91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "modelfile = os.path.join('.','20201008-140436_model_100-5-100-0.bin')\n",
    "plot_embeding(modelfile, Nsamples=Nsamples, positives=positives, emojis=emojis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Word2Vec (caso 3)\n",
    "\n",
    "Hiperparámetros:\n",
    " \n",
    " | size | window | min_count | sg |\n",
    " | --- | --- | --- | --- |\n",
    " | 100  | 5      | 300       | 0 |\n",
    "\n",
    "\n",
    "En el presente caso reduciremos el tamaño del vocabulario mediante el hiperparámetro `min_count = 300`. En primera instancia observamos que las palabras y emojis:\n",
    "\n",
    "* nailed\n",
    "* lovely\n",
    "* beautiful\n",
    "* :grinning_face_with_smiling_eyes:\n",
    "* :grinning_face_with_smiling_eyes:\n",
    "* :grinning_face:\n",
    "\n",
    "quedan fuera del focabulario del embedding y por consiguiente las removemos. Dicho esto, el vocabulario del embedding es de 1400 palabras.\n",
    "\n",
    "Si observamos el scatter plot del embedding par aun un sub conjunto de `Nsamples = 200` palabras podemos identificar nuevamente en el centro un conglomerado palabras pero con significados diferentes. A su vez, las palabras y emojis con connotacion positiva se encuentra mas distanciadas de este conglomeradoy se encuentran muy próximas entre si."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95m./20201008-140513_model_100-5-300-0.bin - Tamaño del vocabulario 1400\u001b[0m\n",
      "\u001b[92mPrimeras 50 compoentes del vocabulario\u001b[0m\n",
      "\u001b[94m['<url>', 'hey', '!', 'welcome', 'yup', '', 'looking', 'problem', \"'ve\", 'reviewed', 'finding', 'domain', 'square', 'root', 'function', 'let', \"'s\", 'work', 'together', 'find', 'exactly', \"'re\", 'stuck', 'tried', '?', 'simplifying', 'wrong', 'okay', 'actually', 'need', 'simplify', 'however', 'would', \"n't\", 'please', 'show', 'check', 'appreciate', 'seem', 'discuss', 'move', 'tell', 'mean', 'word', '\"', 'meant', 'learned', 'far', 'x', 'good']\u001b[0m\n",
      "\u001b[92mUltimas 50 compoentes del vocabulario\u001b[0m\n",
      "\u001b[94m['leslie', 'block', 'forces', 'nguyen', 'covered', 'matrix', 'conjugate', 'radians', 'dy', 'dx', 'week', 'du', 'voltage', 'ms', 'charge', 'rt', 'bond', 'level', ':raising_hands:', 'reflection', 'anna', 'alexander', '…', 'nardo', 'mahato', 'ms.', ':folded_hands:', 'yupppppp', 'tips', 'dt', 'usha', 'sharma', ':smiling_face_with_sunglasses:', '¡', 'di', ':camera:', 'raman', 'pi/2', 'xymines', 'naw', 'woohoo', 'kushwaha', 'kamireddy', 'atomic', 'varma', 'electron', 'eliza', 'bonds', 'molecule', 'shinde']\u001b[0m\n",
      "\u001b[91m#######################################\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78dbefff1d5b42b28f89c53c673cc22b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "positives = list()\n",
    "positives.append('great')\n",
    "positives.append('excellent')\n",
    "positives.append('nice')\n",
    "positives.append('awesome')\n",
    "positives.append('amazing')\n",
    "positives.append('fantastic')\n",
    "positives.append('wonderful')\n",
    "positives.append('perfect')\n",
    "# positives.append('nailed')\n",
    "positives.append('brilliant')\n",
    "# positives.append('lovely')\n",
    "positives.append('superb')\n",
    "# positives.append('beautiful')\n",
    "positives.append('yupppppp')\n",
    "\n",
    "emojis = list()\n",
    "# emojis.append(':grinning_face_with_smiling_eyes:')\n",
    "emojis.append(':smiling_face_with_smiling_eyes:')\n",
    "emojis.append(':face_with_tears_of_joy:')\n",
    "# emojis.append(':grinning_face:')\n",
    "emojis.append(':smiling_face_with_sunglasses:')\n",
    "emojis.append(':smiling_face_with_open_mouth_&_smiling_eyes:')\n",
    "\n",
    "modelfile = os.path.join('.','20201008-140513_model_100-5-300-0.bin')\n",
    "plot_embeding(modelfile, Nsamples=Nsamples, positives=positives, emojis=emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfile = os.path.join('.','20201008-142337_model_100-1-300-0.bin')\n",
    "plot_embeding(modelfile, Nsamples=Nsamples, positives=positives, emojis=emojis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove_input_file = '../glove.6B/glove.6B.100d.txt'\n",
    "# word2vec_output_file = '../glove.6B/glove.6B.100d.txt.word2vec'\n",
    "# glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives = ['thanks','great', 'excellent', 'nice', 'awesome', 'amazing', 'fantastic', 'wonderful', 'perfect', 'nailed', 'brilliant', 'lovely', 'superb', 'beautiful'] #, 'yupppppp'\n",
    "# emojis = [':grinning_face_with_smiling_eyes:', ':smiling_face_with_smiling_eyes:', ':face_with_tears_of_joy:', ':grinning_face:', ':smiling_face_with_sunglasses:',':smiling_face_with_open_mouth_&_smiling_eyes:']\n",
    "\n",
    "modelfile = os.path.join('..', 'glove.6B', 'glove.6B.100d.txt.word2vec')\n",
    "plot_embeding(modelfile, Nsamples=Nsamples, positives=positives, emojis=None, useglove=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('diplo': venv)",
   "language": "python",
   "name": "python37664bitdiplovenvc8f1cc3d26344ed08febef30f9915ee6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
