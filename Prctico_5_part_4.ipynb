{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.0 64-bit",
   "display_name": "Python 3.7.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "c0558758b2c20aded1bec288d8b5d95f0ce82cc10523c91486ccff78ac667d20"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Practico 5 (parte 4)\n",
    "\n",
    "## Entrenar word embeddings\n",
    "\n",
    "## Importación de módulos y librerías"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inclusion de librerias y módulos\n",
    "import os\n",
    "import logging\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Usamos las stopwords definidas en nltk más algunas propias\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = stopwords.words('english') + [',', \"’\", '.', ':', '-', ';']\n",
    "\n",
    "# Algunas utilidades\n",
    "from utiles import print_some_info\n",
    "\n",
    "# Nos permite convertir str a list\n",
    "from ast import literal_eval\n",
    "\n",
    "# Importamos wrod2vec de la lib gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Importamos logger para tener informacion de estado\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Colores\n",
    "BLUE   = '#5DADE2'\n",
    "RED    = '#ff7043'\n",
    "ORANGE = '#F5B041'\n",
    "GREEN  = '#58D68D'\n",
    "YELLOW = '#F4D03F'\n",
    "pltcolors = [BLUE, RED, ORANGE, GREEN, YELLOW]\n",
    "\n",
    "# Plot axes y legends parambs\n",
    "plt.rcParams[\"axes.labelweight\"]   = \"bold\"\n",
    "plt.rcParams[\"axes.titleweight\"]   = \"bold\"\n",
    "plt.rcParams[\"legend.shadow\"]      = True\n",
    "plt.rcParams[\"figure.titleweight\"] = \"bold\"\n",
    "\n",
    "data_dir = os.path.join('..', 'dataset')\n",
    "\n",
    "SAVE_CURATED_DATASET = True"
   ]
  },
  {
   "source": [
    "## Lectura del archivo de mensajes\n",
    "Utilizamos unicamente el archivo de mensajes dado que vamos a entrenar un word embeding como word2vec. Entendemos que para el propósito del análisis y por que no estamos empleando ningún modelo de clasificación o regresión podemos usar el conjunto de datos completo."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "El conjunto de datos utilizado es dev_yup_messages_preprocessed.csv\nEl conjunto de datos posee 234375 filas y 6 columnas\n&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;\nRangeIndex: 234375 entries, 0 to 234374\nData columns (total 6 columns):\n #   Column        Non-Null Count   Dtype \n---  ------        --------------   ----- \n 0   session_id    234375 non-null  int64 \n 1   created_at    234375 non-null  object\n 2   sent_from     234375 non-null  object\n 3   sent_to       234375 non-null  object\n 4   content_type  234375 non-null  object\n 5   text          234375 non-null  object\ndtypes: int64(1), object(5)\nmemory usage: 10.7+ MB\nNone\n"
    }
   ],
   "source": [
    "filename = 'dev_yup_messages_preprocessed.csv'\n",
    "df = pd.read_csv(os.path.join(data_dir, filename))\n",
    "\n",
    "print(f'El conjunto de datos utilizado es {filename}')\n",
    "print_some_info(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emot\n",
    "from emot.emo_unicode import UNICODE_EMO, EMOTICONS\n",
    "\n",
    "# Funcion para convertir emojis a palabras\n",
    "def convert_emojis(text):\n",
    "    text0 = [t for t in re.findall(r'\\\\x..\\\\x..\\\\x..\\\\x..', str(text.encode()))]\n",
    "    text0 = [''.join(t.split('\\\\x')[1:]) for t in text0]\n",
    "    text0 = [bytes.fromhex(t).decode() for t in text0 if t[0]=='f']\n",
    "    text0 = [UNICODE_EMO[t] for t in text0 if t in UNICODE_EMO]\n",
    "    text0 = ' '.join(text0)\n",
    "    return text0 if text0 else text\n",
    "\n",
    "# Funcion para convertir emoticones a palabras\n",
    "OUREMOTIC = dict([(e, f\":{EMOTICONS[e].lower().split(',')[0].replace('or ','').replace(' ','_')}:\") for e in EMOTICONS.keys()])\n",
    "def convert_emoticons(text):\n",
    "    try:\n",
    "        text0 = emot.emoticons(text)\n",
    "        if text0['flag']:\n",
    "            return ':'+text0['mean'][0].replace(' ', '_').lower()+':'\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "El conjunto de datos posee 210242 filas y 3 columnas\n&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;\nInt64Index: 210242 entries, 0 to 234374\nData columns (total 3 columns):\n #   Column      Non-Null Count   Dtype \n---  ------      --------------   ----- \n 0   session_id  210242 non-null  int64 \n 1   sent_from   210242 non-null  object\n 2   text        210242 non-null  object\ndtypes: int64(1), object(2)\nmemory usage: 6.4+ MB\nNone\n"
    }
   ],
   "source": [
    "#1. Tomamos solo las columnas que nos pueden servir. Esto es preliminar, podríamos tomar solo `text`\n",
    "dfclean = df[['session_id', 'sent_from', 'text']]\n",
    "\n",
    "#2. Tomamos solo las filas que sean tutor o student a partir de la columna `sent_from`\n",
    "dfclean = dfclean[dfclean.sent_from.isin(['student', 'tutor'])]\n",
    "\n",
    "#3. Convertimos a lista de strings el contenido de la columna text\n",
    "dfclean['text'] = dfclean.text.apply(lambda x: literal_eval(x))\n",
    "\n",
    "#4. Se sustituyen emojis por tokens \n",
    "dfclean['text'] = dfclean.text.apply(lambda x: [convert_emojis(w) for w in x])\n",
    "\n",
    "#5. Se sustituyen emoticones por palabras\n",
    "## No lo vamos a tratar por ahora por que requiere de un mejor tratamiento. \n",
    "# Los parentesis, llaves y corchetes parece que el uso regular afecta al manejo del emoticon. \n",
    "# dfclean['text'] = dfclean.text.apply(lambda x: [convert_emoticons(w) for w in x])\n",
    "\n",
    "#6. Convernitimos a minúsculas para unificar el tratamiento\n",
    "dfclean['text'] = dfclean.text.apply(lambda x: [w.lower() for w in x])\n",
    "\n",
    "#7. Removemos las stopwords\n",
    "dfclean['text'] = dfclean.text.apply(lambda x: [w for w in x if w not in stopwords])\n",
    "\n",
    "print_some_info(dfclean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "2020-10-06 19:30:34,939 : INFO : collecting all words and their counts\n2020-10-06 19:30:34,940 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n2020-10-06 19:30:34,959 : INFO : PROGRESS: at sentence #10000, processed 42888 words, keeping 3498 word types\n2020-10-06 19:30:34,973 : INFO : PROGRESS: at sentence #20000, processed 84921 words, keeping 5278 word types\n2020-10-06 19:30:34,988 : INFO : PROGRESS: at sentence #30000, processed 126737 words, keeping 6775 word types\n2020-10-06 19:30:35,004 : INFO : PROGRESS: at sentence #40000, processed 167356 words, keeping 8082 word types\n2020-10-06 19:30:35,021 : INFO : PROGRESS: at sentence #50000, processed 208696 words, keeping 9217 word types\n2020-10-06 19:30:35,037 : INFO : PROGRESS: at sentence #60000, processed 247775 words, keeping 10307 word types\n2020-10-06 19:30:35,053 : INFO : PROGRESS: at sentence #70000, processed 288680 words, keeping 11328 word types\n2020-10-06 19:30:35,067 : INFO : PROGRESS: at sentence #80000, processed 327855 words, keeping 12544 word types\n2020-10-06 19:30:35,081 : INFO : PROGRESS: at sentence #90000, processed 365169 words, keeping 13561 word types\n2020-10-06 19:30:35,095 : INFO : PROGRESS: at sentence #100000, processed 404264 words, keeping 14455 word types\n2020-10-06 19:30:35,109 : INFO : PROGRESS: at sentence #110000, processed 443711 words, keeping 15458 word types\n2020-10-06 19:30:35,123 : INFO : PROGRESS: at sentence #120000, processed 482818 words, keeping 16412 word types\n2020-10-06 19:30:35,138 : INFO : PROGRESS: at sentence #130000, processed 522083 words, keeping 17399 word types\n2020-10-06 19:30:35,152 : INFO : PROGRESS: at sentence #140000, processed 562129 words, keeping 18382 word types\n2020-10-06 19:30:35,169 : INFO : PROGRESS: at sentence #150000, processed 600320 words, keeping 19278 word types\n2020-10-06 19:30:35,183 : INFO : PROGRESS: at sentence #160000, processed 639050 words, keeping 20101 word types\n2020-10-06 19:30:35,197 : INFO : PROGRESS: at sentence #170000, processed 677385 words, keeping 20848 word types\n2020-10-06 19:30:35,212 : INFO : PROGRESS: at sentence #180000, processed 717710 words, keeping 21696 word types\n2020-10-06 19:30:35,227 : INFO : PROGRESS: at sentence #190000, processed 757131 words, keeping 22521 word types\n2020-10-06 19:30:35,242 : INFO : PROGRESS: at sentence #200000, processed 797496 words, keeping 23345 word types\n2020-10-06 19:30:35,257 : INFO : PROGRESS: at sentence #210000, processed 837252 words, keeping 24073 word types\n2020-10-06 19:30:35,258 : INFO : collected 24091 word types from a corpus of 838230 raw words and 210242 sentences\n2020-10-06 19:30:35,259 : INFO : Loading a fresh vocabulary\n2020-10-06 19:30:35,320 : INFO : effective_min_count=1 retains 24091 unique words (100% of original 24091, drops 0)\n2020-10-06 19:30:35,321 : INFO : effective_min_count=1 leaves 838230 word corpus (100% of original 838230, drops 0)\n2020-10-06 19:30:35,441 : INFO : deleting the raw counts dictionary of 24091 items\n2020-10-06 19:30:35,443 : INFO : sample=0.001 downsamples 68 most-common words\n2020-10-06 19:30:35,445 : INFO : downsampling leaves estimated 635002 word corpus (75.8% of prior 838230)\n2020-10-06 19:30:35,510 : INFO : estimated required memory for 24091 words and 100 dimensions: 31318300 bytes\n2020-10-06 19:30:35,511 : INFO : resetting layer weights\n2020-10-06 19:30:41,864 : INFO : training model with 4 workers on 24091 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n2020-10-06 19:30:42,453 : INFO : worker thread finished; awaiting finish of 3 more threads\n2020-10-06 19:30:42,456 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-10-06 19:30:42,460 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-10-06 19:30:42,462 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-10-06 19:30:42,463 : INFO : EPOCH - 1 : training on 838230 raw words (634872 effective words) took 0.6s, 1076958 effective words/s\n2020-10-06 19:30:43,022 : INFO : worker thread finished; awaiting finish of 3 more threads\n2020-10-06 19:30:43,024 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-10-06 19:30:43,027 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-10-06 19:30:43,030 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-10-06 19:30:43,030 : INFO : EPOCH - 2 : training on 838230 raw words (634799 effective words) took 0.6s, 1132319 effective words/s\n2020-10-06 19:30:43,651 : INFO : worker thread finished; awaiting finish of 3 more threads\n2020-10-06 19:30:43,653 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-10-06 19:30:43,660 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-10-06 19:30:43,662 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-10-06 19:30:43,662 : INFO : EPOCH - 3 : training on 838230 raw words (634882 effective words) took 0.6s, 1015709 effective words/s\n2020-10-06 19:30:44,287 : INFO : worker thread finished; awaiting finish of 3 more threads\n2020-10-06 19:30:44,291 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-10-06 19:30:44,296 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-10-06 19:30:44,298 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-10-06 19:30:44,298 : INFO : EPOCH - 4 : training on 838230 raw words (635026 effective words) took 0.6s, 1010426 effective words/s\n2020-10-06 19:30:44,905 : INFO : worker thread finished; awaiting finish of 3 more threads\n2020-10-06 19:30:44,908 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-10-06 19:30:44,913 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-10-06 19:30:44,914 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-10-06 19:30:44,914 : INFO : EPOCH - 5 : training on 838230 raw words (634809 effective words) took 0.6s, 1042208 effective words/s\n2020-10-06 19:30:44,915 : INFO : training on a 4191150 raw words (3174388 effective words) took 3.0s, 1040994 effective words/s\n"
    }
   ],
   "source": [
    "model = Word2Vec(list(dfclean.text), size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "2020-10-06 19:37:45,025 : INFO : precomputing L2-norms of word weight vectors\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[(&#39;thank&#39;, 0.8911322355270386),\n (&#39;thx&#39;, 0.7975543737411499),\n (&#39;appreciate&#39;, 0.7357184886932373),\n (&#39;glad&#39;, 0.7197262048721313),\n (&#39;thks&#39;, 0.7142689228057861),\n (&#39;thankyou&#39;, 0.7096688747406006),\n (&#39;bye&#39;, 0.6871362924575806),\n (&#39;love&#39;, 0.6796211004257202),\n (&#39;accuracy&#39;, 0.679126501083374),\n (&#39;hear&#39;, 0.6663104295730591)]"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "model.most_similar('thanks')"
   ]
  }
 ]
}